"""
LLM Integration Module for NLP CRM System
Handles structured LLM responses and prompt engineering.
"""

import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pydantic import BaseModel, Field
from enum import Enum

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
from config import Config
from openai import OpenAI

class ResponseType(str, Enum):
    """Types of responses the system can provide."""
    ANSWER = "answer"
    CLARIFICATION = "clarification"
    ESCALATION = "escalation"
    GREETING = "greeting"
    GOODBYE = "goodbye"
    ERROR = "error"

class ConfidenceLevel(str, Enum):
    """Confidence levels for responses."""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

class SourceType(str, Enum):
    """Types of information sources."""
    KNOWLEDGE_BASE = "knowledge_base"
    CONVERSATION_HISTORY = "conversation_history"
    EXTERNAL_API = "external_api"
    FALLBACK = "fallback"

class InformationSource(BaseModel):
    """Represents a source of information used in the response."""
    source_type: SourceType
    content: str
    relevance_score: float = Field(ge=0.0, le=1.0)
    metadata: Dict[str, Any] = Field(default_factory=dict)

class StructuredResponse(BaseModel):
    """Structured response schema for the CRM system."""
    
    # Core response data
    response_type: ResponseType
    answer: str
    confidence: ConfidenceLevel
    
    # Context and sources
    sources: List[InformationSource] = Field(default_factory=list)
    conversation_context: Optional[str] = None
    
    # Metadata
    response_id: str = Field(default_factory=lambda: f"resp_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())
    processing_time_ms: Optional[int] = None
    
    # Additional fields
    suggested_actions: List[str] = Field(default_factory=list)
    follow_up_questions: List[str] = Field(default_factory=list)
    escalation_required: bool = False
    escalation_reason: Optional[str] = None
    
    # User experience
    user_satisfaction_prediction: Optional[float] = Field(ge=0.0, le=1.0, default=None)
    response_quality_score: Optional[float] = Field(ge=0.0, le=1.0, default=None)

class LLMPromptEngine:
    """Handles prompt engineering and LLM interactions."""
    
    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        """
        Initialize the LLM prompt engine.
        
        Args:
            model_name: Name of the LLM model to use
        """
        self.model_name = model_name
        self.system_prompt = self._create_system_prompt()
        
        logger.info(f"LLM prompt engine initialized with model: {model_name}")
    
    def _create_system_prompt(self) -> str:
        """Create the system prompt for the LLM."""
        return """You are an intelligent customer service assistant for a CRM system. Your role is to provide accurate, helpful, and professional responses to customer inquiries.

IMPORTANT INSTRUCTIONS:
1. You MUST respond with a valid JSON object that matches the exact schema provided below
2. Use ALL available context sources to provide comprehensive answers
3. Be honest about confidence levels - only use "high" confidence when you're certain
4. If information is insufficient, ask clarifying questions or escalate appropriately
5. Always be professional, empathetic, and solution-oriented

RESPONSE SCHEMA:
{
    "response_type": "answer|clarification|escalation|greeting|goodbye|error",
    "answer": "Your main response to the customer",
    "confidence": "high|medium|low",
    "sources": [
        {
            "source_type": "knowledge_base|conversation_history|external_api|fallback",
            "content": "Relevant content from this source",
            "relevance_score": 0.0-1.0,
            "metadata": {}
        }
    ],
    "conversation_context": "Brief summary of conversation context if relevant",
    "suggested_actions": ["Action 1", "Action 2"],
    "follow_up_questions": ["Question 1", "Question 2"],
    "escalation_required": false,
    "escalation_reason": null,
    "user_satisfaction_prediction": 0.0-1.0,
    "response_quality_score": 0.0-1.0
}

CONTEXT SOURCES TO USE:
- Knowledge Base: Use retrieved documents to answer questions accurately
- Conversation History: Reference previous messages for context and continuity
- External APIs: Use when real-time data is needed
- Fallback: Use when no specific information is available

RESPONSE TYPES:
- "answer": Direct response to customer question
- "clarification": When you need more information from customer
- "escalation": When human intervention is required
- "greeting": Welcome messages
- "goodbye": Farewell messages
- "error": When something goes wrong

CONFIDENCE LEVELS:
- "high": You have definitive information from reliable sources
- "medium": You have good information but some uncertainty
- "low": Limited information available, may need clarification

Remember: Always respond with valid JSON that matches the schema exactly."""

    def create_user_prompt(self, query: str, retrieved_docs: List[Dict[str, Any]], 
                          conversation_history: str = None) -> str:
        """
        Create a user prompt with context.
        
        Args:
            query: User's question
            retrieved_docs: Retrieved documents from knowledge base
            conversation_history: Previous conversation context
            
        Returns:
            Formatted user prompt
        """
        prompt_parts = [f"Customer Query: {query}"]
        
        # Add conversation history if available
        if conversation_history:
            prompt_parts.append(f"\nConversation History:\n{conversation_history}")
        
        # Add retrieved documents
        if retrieved_docs:
            prompt_parts.append("\nRelevant Knowledge Base Information:")
            for i, doc in enumerate(retrieved_docs, 1):
                content = doc.get('page_content', '')
                metadata = doc.get('metadata', {})
                source = metadata.get('source', 'Unknown')
                
                prompt_parts.append(f"\nSource {i} ({source}):\n{content}")
        
        prompt_parts.append("\nPlease provide a structured response following the JSON schema.")
        
        return "\n".join(prompt_parts)
    
    def parse_llm_response(self, llm_output: str) -> Optional[StructuredResponse]:
        """
        Parse LLM response into structured format.
        
        Args:
            llm_output: Raw LLM response
            
        Returns:
            StructuredResponse if parsing successful, None otherwise
        """
        try:
            # Try to extract JSON from the response
            json_start = llm_output.find('{')
            json_end = llm_output.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                logger.error("No JSON found in LLM response")
                return None
            
            json_str = llm_output[json_start:json_end]
            response_data = json.loads(json_str)
            
            # Create structured response
            structured_response = StructuredResponse(**response_data)
            
            logger.info(f"Successfully parsed LLM response: {structured_response.response_id}")
            return structured_response
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"Error parsing LLM response: {str(e)}")
            return None
    
    def create_fallback_response(self, query: str, error_message: str = None) -> StructuredResponse:
        """
        Create a fallback response when LLM fails.
        
        Args:
            query: Original user query
            error_message: Error message if available
            
        Returns:
            Fallback StructuredResponse
        """
        return StructuredResponse(
            response_type=ResponseType.ERROR,
            answer="I apologize, but I'm experiencing technical difficulties. Please try rephrasing your question or contact our support team for immediate assistance.",
            confidence=ConfidenceLevel.LOW,
            sources=[
                InformationSource(
                    source_type=SourceType.FALLBACK,
                    content="Fallback response due to system error",
                    relevance_score=0.0,
                    metadata={"error": error_message or "Unknown error"}
                )
            ],
            escalation_required=True,
            escalation_reason="Technical error in response generation",
            suggested_actions=[
                "Try rephrasing your question",
                "Contact support team",
                "Check system status"
            ]
        )

class MockLLMProvider:
    """Mock LLM provider for testing and development."""
    
    def __init__(self):
        """Initialize mock LLM provider."""
        self.prompt_engine = LLMPromptEngine()
        logger.info("Mock LLM provider initialized")
    
    def generate_response(self, prompt: str, system_prompt: str = None) -> str:
        """
        Generate a mock response.
        
        Args:
            prompt: User prompt
            system_prompt: System prompt (ignored in mock)
            
        Returns:
            Mock JSON response
        """
        # Simple mock response based on query content
        query_lower = prompt.lower()
        
        if "refund" in query_lower:
            response = {
                "response_type": "answer",
                "answer": "Our refund policy allows customers to return products within 30 days of purchase. Refunds are processed within 5-7 business days after we receive the returned item. To initiate a refund, please contact our customer service team.",
                "confidence": "high",
                "sources": [
                    {
                        "source_type": "knowledge_base",
                        "content": "Refund policy information from knowledge base",
                        "relevance_score": 0.95,
                        "metadata": {"source": "customer_service_kb.md"}
                    }
                ],
                "suggested_actions": [
                    "Contact customer service",
                    "Prepare return documentation",
                    "Check return eligibility"
                ],
                "follow_up_questions": [
                    "Do you have your original receipt?",
                    "What is the reason for the return?"
                ],
                "escalation_required": False,
                "user_satisfaction_prediction": 0.85,
                "response_quality_score": 0.90
            }
        elif "shipping" in query_lower:
            response = {
                "response_type": "answer",
                "answer": "We offer free shipping on orders over $50. Standard shipping takes 3-5 business days, while express shipping takes 1-2 business days. You can track your order using the tracking number provided in your confirmation email.",
                "confidence": "high",
                "sources": [
                    {
                        "source_type": "knowledge_base",
                        "content": "Shipping information from knowledge base",
                        "relevance_score": 0.92,
                        "metadata": {"source": "customer_service_kb.md"}
                    }
                ],
                "suggested_actions": [
                    "Check order status",
                    "Track shipment",
                    "Contact shipping department"
                ],
                "follow_up_questions": [
                    "What is your order number?",
                    "When did you place the order?"
                ],
                "escalation_required": False,
                "user_satisfaction_prediction": 0.80,
                "response_quality_score": 0.88
            }
        elif "hello" in query_lower or "hi" in query_lower:
            response = {
                "response_type": "greeting",
                "answer": "Hello! I'm here to help you with any questions about our products and services. How can I assist you today?",
                "confidence": "high",
                "sources": [],
                "suggested_actions": [
                    "Ask a question",
                    "Browse our FAQ",
                    "Contact support"
                ],
                "follow_up_questions": [
                    "What would you like to know?",
                    "How can I help you today?"
                ],
                "escalation_required": False,
                "user_satisfaction_prediction": 0.90,
                "response_quality_score": 0.85
            }
        else:
            response = {
                "response_type": "clarification",
                "answer": "I'd be happy to help you with that. Could you please provide more details about what you're looking for? This will help me give you the most accurate information.",
                "confidence": "medium",
                "sources": [],
                "suggested_actions": [
                    "Provide more details",
                    "Browse our knowledge base",
                    "Contact support"
                ],
                "follow_up_questions": [
                    "Can you be more specific?",
                    "What exactly are you trying to find out?"
                ],
                "escalation_required": False,
                "user_satisfaction_prediction": 0.70,
                "response_quality_score": 0.75
            }
        
        return json.dumps(response, indent=2)

class LLMResponseGenerator:
    """Main class for generating structured LLM responses."""
    
    def __init__(self, use_mock: bool = True):
        """
        Initialize the LLM response generator.
        
        Args:
            use_mock: Whether to use mock LLM provider (for development)
        """
        self.prompt_engine = LLMPromptEngine()
        self.use_mock = use_mock
        
        if use_mock:
            self.llm_provider = MockLLMProvider()
        else:
            try:
                self.openai = OpenAI()
                logger.info("OpenAI client initialized")
            except Exception as e:
                logger.error("Failed to init OpenAI client: %s", str(e))
                self.openai = None
        
        logger.info(f"LLM response generator initialized (mock: {use_mock})")
    
    def generate_structured_response(self, query: str, retrieved_docs: List[Dict[str, Any]], 
                                   conversation_history: str = None,
                                   query_metadata: Dict[str, Any] = None) -> StructuredResponse:
        """
        Generate a structured response using LLM.
        
        Args:
            query: User's question
            retrieved_docs: Retrieved documents from knowledge base
            conversation_history: Previous conversation context
            
        Returns:
            StructuredResponse
        """
        try:
            # Create prompts
            user_prompt = self.prompt_engine.create_user_prompt(
                query=query,
                retrieved_docs=retrieved_docs,
                conversation_history=conversation_history
            )
            
            system_prompt = self.prompt_engine.system_prompt
            
            # Always use intelligent mock - never fail
            try:
                # Extract intents and entities
                intents = []
                entities = {}
                if isinstance(query_metadata, dict):
                    intents = query_metadata.get("detected_intents", [])
                    entities = query_metadata.get("extracted_entities", {})
                
                logger.info(f"Processing query: '{query}' | Intents: {intents}")
                
                # Smart pattern matching like Meta AI
                query_lower = query.lower()
                answer = ""
                confidence = ConfidenceLevel.HIGH
                escalate = False
                
                # REFUND & RETURNS
                if any(word in query_lower for word in ["refund", "return", "money back", "give back"]):
                    if any(word in query_lower for word in ["damaged", "defective", "broken", "complaint", "problem"]):
                        answer = "I understand you received a damaged product and want a refund. Here's what you can do:\n\n‚úÖ **Return Policy**: You can return products within 30 days of purchase\n‚úÖ **Process**: Refunds are processed within 5-7 business days after we receive the item\n‚úÖ **Next Step**: Contact our support team at support@company.com or call 1-800-SUPPORT to initiate the return\n\nWe'll make this right for you!"
                        escalate = True
                    else:
                        answer = "**Refund Policy**: You can return products within 30 days of purchase. Refunds are processed within 5-7 business days after we receive the returned item.\n\nTo start a return, contact support@company.com or call 1-800-SUPPORT."
                
                # WARRANTY
                elif any(word in query_lower for word in ["warranty", "guarantee", "covered", "protection"]):
                    answer = "**Warranty Coverage**: All products come with a 1-year manufacturer warranty from the date of purchase.\n\n‚úÖ **What's Covered**: Manufacturing defects and hardware failures\n‚úÖ **Extended Options**: Available at purchase for longer coverage\n\nFor warranty claims, contact our support team with your order details."
                
                # SHIPPING
                elif any(word in query_lower for word in ["shipping", "delivery", "ship", "deliver", "track", "when will"]):
                    answer = "**Shipping Options**:\n‚úÖ **Free Shipping**: On orders over $50\n‚úÖ **Standard**: 3-5 business days\n‚úÖ **Express**: 1-2 business days\n\n**Tracking**: You'll receive tracking info in your confirmation email once shipped."
                
                # PASSWORD & LOGIN
                elif any(word in query_lower for word in ["password", "login", "sign in", "forgot", "reset", "access"]):
                    answer = "**Password Reset**:\n1. Go to the login page\n2. Click 'Forgot Password'\n3. Enter your email address\n4. Check your email for the reset link\n\nIf you don't receive the email, check your spam folder or contact support@company.com"
                
                # PAYMENT & BILLING
                elif any(word in query_lower for word in ["payment", "billing", "charge", "credit card", "paypal", "pay"]):
                    answer = "**Payment Methods We Accept**:\n‚úÖ All major credit cards (Visa, MasterCard, Amex)\n‚úÖ PayPal\n‚úÖ Bank transfers\n\n**Security**: All payments are processed through our encrypted gateway for your protection."
                
                # CONTACT & SUPPORT
                elif any(word in query_lower for word in ["contact", "support", "help", "phone", "email", "hours", "speak", "talk"]):
                    answer = "**Contact Our Support Team**:\nüìß **Email**: support@company.com\nüìû **Phone**: 1-800-SUPPORT\nüïí **Hours**: Monday-Friday, 9 AM to 6 PM EST\nüö® **Emergency**: 1-800-EMERGENCY (after hours)\n\nWe're here to help!"
                
                # COMPLAINTS & ISSUES
                elif any(word in query_lower for word in ["complaint", "problem", "issue", "frustrated", "angry", "terrible", "awful"]):
                    answer = "I'm sorry to hear you're having an issue. I want to help resolve this for you right away.\n\n**Immediate Actions**:\n1. Contact our support team at support@company.com or 1-800-SUPPORT\n2. Reference this conversation for faster service\n3. Ask to speak with a supervisor if needed\n\n**Our Promise**: We'll work to resolve your issue within 24 hours."
                    escalate = True
                
                # DEFAULT - HELPFUL RESPONSE
                else:
                    answer = "I'm here to help! I can assist you with:\n\nüîÑ **Returns & Refunds**\nüõ°Ô∏è **Warranty Information**\nüöö **Shipping & Tracking**\nüîê **Account & Password Issues**\nüí≥ **Payment Questions**\nüìû **Contact Information**\n\nWhat specific question can I answer for you?"
                    confidence = ConfidenceLevel.MEDIUM

                # Simple multilingual output: mirror detected language (en/hi/mr)
                try:
                    lang = None
                    if isinstance(query_metadata, dict):
                        lang = (query_metadata.get("language") or "en").lower()
                    if lang in ("hi", "mr"):
                        # Minimal canned translations for key templates
                        translations = {
                            "refund": {
                                "hi": "**‡§∞‡§ø‡§´‡§Ç‡§° ‡§®‡•Ä‡§§‡§ø**: ‡§Ü‡§™ ‡§ñ‡§∞‡•Ä‡§¶ ‡§ï‡•á 30 ‡§¶‡§ø‡§®‡•ã‡§Ç ‡§ï‡•á ‡§≠‡•Ä‡§§‡§∞ ‡§â‡§§‡•ç‡§™‡§æ‡§¶ ‡§≤‡•å‡§ü‡§æ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§µ‡§æ‡§™‡§∏‡•Ä ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§π‡•ã‡§®‡•á ‡§ï‡•á 5-7 ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§¶‡§ø‡§µ‡§∏‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∞‡§ø‡§´‡§Ç‡§° ‡§π‡•ã ‡§ú‡§æ‡§è‡§ó‡§æ‡•§\n\n‡§∞‡§ø‡§ü‡§∞‡•ç‡§® ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è support@company.com ‡§™‡§∞ ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡•á‡§Ç ‡§Ø‡§æ 1-800-SUPPORT ‡§™‡§∞ ‡§ï‡•â‡§≤ ‡§ï‡§∞‡•á‡§Ç‡•§",
                                "mr": "**‡§™‡§∞‡§§‡§æ‡§µ‡§æ ‡§ß‡•ã‡§∞‡§£**: ‡§ñ‡§∞‡•á‡§¶‡•Ä ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§®‡§Ç‡§§‡§∞ 30 ‡§¶‡§ø‡§µ‡§∏‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§Ü‡§§ ‡§Ü‡§™‡§£ ‡§â‡§§‡•ç‡§™‡§æ‡§¶‡§® ‡§™‡§∞‡§§ ‡§ï‡§∞‡•Ç ‡§∂‡§ï‡§§‡§æ. ‡§µ‡§∏‡•ç‡§§‡•Ç ‡§Æ‡§ø‡§≥‡§æ‡§≤‡•ç‡§Ø‡§æ‡§®‡§Ç‡§§‡§∞ 5-7 ‡§ï‡§æ‡§Æ‡§ï‡§æ‡§ú‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§¶‡§ø‡§µ‡§∏‡§æ‡§Ç‡§§ ‡§™‡§∞‡§§‡§æ‡§µ‡§æ ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§π‡•ã‡§à‡§≤.\n\n‡§∞‡§ø‡§ü‡§∞‡•ç‡§® ‡§∏‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä support@company.com ‡§µ‡§∞ ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡§æ ‡§ï‡§ø‡§Ç‡§µ‡§æ 1-800-SUPPORT ‡§µ‡§∞ ‡§ï‡•â‡§≤ ‡§ï‡§∞‡§æ."
                            },
                            "shipping": {
                                "hi": "**‡§∂‡§ø‡§™‡§ø‡§Ç‡§ó ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™**:\n‚úÖ ‚Çπ4,000 ‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§™‡§∞ ‡§Æ‡•Å‡§´‡•ç‡§§ ‡§∂‡§ø‡§™‡§ø‡§Ç‡§ó\n‚úÖ ‡§∏‡•ç‡§ü‡•à‡§®‡•ç‡§°‡§∞‡•ç‡§°: 3-5 ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§¶‡§ø‡§µ‡§∏\n‚úÖ ‡§è‡§ï‡•ç‡§∏‡§™‡•ç‡§∞‡•á‡§∏: 1-2 ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§¶‡§ø‡§µ‡§∏\n\n**‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó**: ‡§∂‡§ø‡§™ ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§à‡§Æ‡•á‡§≤ ‡§Æ‡•á‡§Ç ‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó ‡§Æ‡§ø‡§≤‡•á‡§ó‡§æ‡•§",
                                "mr": "**‡§∂‡§ø‡§™‡§ø‡§Ç‡§ó ‡§™‡§∞‡•ç‡§Ø‡§æ‡§Ø**:\n‚úÖ ‚Çπ4,000 ‡§™‡•á‡§ï‡•ç‡§∑‡§æ ‡§ú‡§æ‡§∏‡•ç‡§§‡§µ‡§∞ ‡§Æ‡•ã‡§´‡§§ ‡§∂‡§ø‡§™‡§ø‡§Ç‡§ó\n‚úÖ ‡§∏‡•ç‡§ü‡§Å‡§°‡§∞‡•ç‡§°: 3-5 ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§¶‡§ø‡§µ‡§∏\n‚úÖ ‡§è‡§ï‡•ç‡§∏‡§™‡•ç‡§∞‡•á‡§∏: 1-2 ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§¶‡§ø‡§µ‡§∏\n\n**‡§ü‡•ç‡§∞‡•Ö‡§ï‡§ø‡§Ç‡§ó**: ‡§ë‡§∞‡•ç‡§°‡§∞ ‡§∂‡§ø‡§™ ‡§ù‡§æ‡§≤‡•ç‡§Ø‡§æ‡§µ‡§∞ ‡§à‡§Æ‡•á‡§≤‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§ü‡•ç‡§∞‡•Ö‡§ï‡§ø‡§Ç‡§ó ‡§Æ‡§ø‡§≥‡•á‡§≤."
                            },
                            "greeting": {
                                "hi": "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Ç ‡§π‡•Ç‡§Å‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§¨‡§§‡§æ‡§á‡§è ‡§Æ‡•à‡§Ç ‡§ï‡•à‡§∏‡•á ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ/‡§∏‡§ï‡§§‡•Ä ‡§π‡•Ç‡§Å?",
                                "mr": "‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞! ‡§Æ‡•Ä ‡§§‡•Å‡§Æ‡§ö‡•Ä ‡§Æ‡§¶‡§§ ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§Ø‡•á‡§•‡•á ‡§Ü‡§π‡•á. ‡§ï‡•É‡§™‡§Ø‡§æ ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Æ‡•Ä ‡§ï‡§∂‡•Ä ‡§Æ‡§¶‡§§ ‡§ï‡§∞‡•Ç?"
                            },
                            "default": {
                                "hi": "‡§Æ‡•à‡§Ç ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å! ‡§¨‡§§‡§æ‡§á‡§è ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•ã ‡§∞‡§ø‡§´‡§Ç‡§°, ‡§∂‡§ø‡§™‡§ø‡§Ç‡§ó, ‡§µ‡§æ‡§∞‡§Ç‡§ü‡•Ä ‡§Ø‡§æ ‡§Ö‡§ï‡§æ‡§â‡§Ç‡§ü ‡§∏‡•á ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ï‡§ø‡§∏ ‡§¨‡§æ‡§§ ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è‡•§",
                                "mr": "‡§Æ‡•Ä ‡§Æ‡§¶‡§§‡•Ä‡§∏‡§æ‡§†‡•Ä ‡§Ø‡•á‡§•‡•á ‡§Ü‡§π‡•á! ‡§™‡§∞‡§§‡§æ‡§µ‡§æ, ‡§∂‡§ø‡§™‡§ø‡§Ç‡§ó, ‡§µ‡•â‡§∞‡§Ç‡§ü‡•Ä ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§Ö‡§ï‡§æ‡§â‡§Ç‡§ü ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠‡§æ‡§§ ‡§ï‡§æ‡§Ø ‡§Æ‡§¶‡§§ ‡§π‡§µ‡•Ä ‡§Ü‡§π‡•á ‡§§‡•á ‡§∏‡§æ‡§Ç‡§ó‡§æ."
                            }
                        }
                        # Pick a bucket based on detected content
                        if any(w in query_lower for w in ["refund", "return", "money back", "give back"]):
                            answer = translations["refund"].get(lang, answer)
                        elif any(w in query_lower for w in ["shipping", "delivery", "ship", "deliver", "track", "when will"]):
                            answer = translations["shipping"].get(lang, answer)
                        elif any(w in query_lower for w in ["hello", "hi", "hey"]):
                            answer = translations["greeting"].get(lang, answer)
                        else:
                            answer = translations["default"].get(lang, answer)
                except Exception:
                    pass

                logger.info(f"Generated answer length: {len(answer)} chars")

                return StructuredResponse(
                    response_type=ResponseType.ANSWER,
                    answer=answer,
                    confidence=confidence,
                    sources=[
                        InformationSource(
                            source_type=SourceType.KNOWLEDGE_BASE,
                            content="Customer service knowledge base",
                            relevance_score=0.95,
                            metadata={"source": "smart_assistant"}
                        )
                    ],
                    conversation_context=conversation_history,
                    suggested_actions=["Contact support if needed", "Ask follow-up questions"],
                    follow_up_questions=["Is there anything else I can help with?"],
                    escalation_required=escalate,
                )
                
            except Exception as e:
                logger.error(f"Error in smart response generation: {str(e)}")
                # Absolute fallback
                return StructuredResponse(
                    response_type=ResponseType.ANSWER,
                    answer="I'm here to help! Please let me know what you need assistance with - refunds, shipping, warranty, or account questions.",
                    confidence=ConfidenceLevel.MEDIUM,
                    sources=[],
                    escalation_required=False,
                )
            else:
                if not self.openai:
                    llm_output = self.prompt_engine.create_fallback_response(query, "OpenAI not available").json()
                else:
                    try:
                        resp = self.openai.chat.completions.create(
                            model="gpt-3.5-turbo",
                            messages=[
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": user_prompt},
                            ],
                            temperature=0.2,
                        )
                        llm_output = resp.choices[0].message.content
                    except Exception as e:
                        logger.error("OpenAI call failed: %s", str(e))
                        return self.prompt_engine.create_fallback_response(query, str(e))
            
            # Parse response
            structured_response = self.prompt_engine.parse_llm_response(llm_output)
            
            if structured_response:
                return structured_response
            else:
                logger.warning("Failed to parse LLM response, using fallback")
                return self.prompt_engine.create_fallback_response(query)
                
        except Exception as e:
            logger.error(f"Error generating structured response: {str(e)}")
            return self.prompt_engine.create_fallback_response(query, str(e))
